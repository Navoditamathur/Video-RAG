{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: youtube_transcript_api in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (0.6.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from youtube_transcript_api) (0.7.1)\n",
      "Requirement already satisfied: requests in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from youtube_transcript_api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->youtube_transcript_api) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->youtube_transcript_api) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->youtube_transcript_api) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->youtube_transcript_api) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pytube in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (15.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: transformers in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (4.47.1)\n",
      "Requirement already satisfied: filelock in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/navoditamathur/.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: google-api-python-client in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (2.138.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-python-client) (2.32.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-python-client) (2.19.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-python-client) (4.1.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (4.25.4)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (1.24.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.31.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (4.9)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client) (0.4.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: yt-dlp in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (2024.12.13)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pinecone-client in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (5.0.1)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from pinecone-client) (2024.2.2)\n",
      "Requirement already satisfied: pinecone-plugin-inference<2.0.0,>=1.0.3 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from pinecone-client) (1.1.0)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from pinecone-client) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Users/navoditamathur/.local/lib/python3.11/site-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (from pinecone-client) (2.2.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: python-dotenv in /Users/navoditamathur/anaconda3/lib/python3.11/site-packages (1.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube_transcript_api\n",
    "!pip install pytube\n",
    "!pip install transformers\n",
    "!pip install google-api-python-client\n",
    "!pip install yt-dlp\n",
    "!pip install pinecone-client\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from pytube import YouTube\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from pytube import Playlist\n",
    "from transformers import pipeline\n",
    "import faiss\n",
    "import numpy as np\n",
    "from googleapiclient.discovery import build\n",
    "from pytube import YouTube, Playlist\n",
    "from faiss import IndexFlatL2\n",
    "from yt_dlp import YoutubeDL\n",
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "\n",
    "# Set up YouTube API\n",
    "developer_api_key = \"YOUR_GOOGLE_DEVELOPER_KEY\"\n",
    "youtube = build(\"youtube\", \"v3\", developerKey=developer_api_key)\n",
    "openai.api_key = 'YOUR_OPEN_API_KEY'\n",
    "\n",
    "pinecone_api_key = 'YOUR_PINECONE_API_KEY'\n",
    "\n",
    "# Initialize CLIP model\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pinecone import Pinecone\n",
    "from googleapiclient.discovery import build\n",
    "import logging\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "from pinecone import ServerlessSpec\n",
    "\n",
    "cloud = os.environ.get('PINECONE_CLOUD') or 'aws'\n",
    "region = os.environ.get('PINECONE_REGION') or 'us-east-1'\n",
    "\n",
    "spec = ServerlessSpec(cloud=cloud, region=region)\n",
    "\n",
    "def init_pinecone(index_name):\n",
    "    pc = Pinecone(\n",
    "        api_key='YOUR_PINECONE_API_KEY'\n",
    "    )\n",
    "\n",
    "    # Now do stuff\n",
    "    if index_name not in pc.list_indexes().names():\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=512,\n",
    "            metric='euclidean',\n",
    "            spec=spec\n",
    "            )\n",
    "    index = pc.Index(index_name)\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to retrieve video IDs from playlist\n",
    "def get_playlist_videos(playlist_url):\n",
    "    playlist = Playlist(playlist_url)\n",
    "    return [video.video_id for video in playlist.videos]\n",
    "\n",
    "def extract_frames(video_path, interval=1, max_frames=500):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    print(cap)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Unable to open video file {video_path}\")\n",
    "        return []\n",
    "    frames = []\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(f\"Error: Unable to read frame from video file {video_path}\")\n",
    "            break\n",
    "        frames.append(frame)\n",
    "        if len(frames) >= max_frames:\n",
    "            break\n",
    "    cap.release()\n",
    "    if len(frames) == 0:\n",
    "        print(f\"No frames extracted from {video_path}.\")\n",
    "    return frames\n",
    "\n",
    "def video_to_images(video_path, output_folder, fps=0.2):\n",
    "    try:\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        clip = VideoFileClip(video_path)\n",
    "        clip.write_images_sequence(\n",
    "            os.path.join(output_folder, \"frame%04d.png\"),\n",
    "            fps=fps\n",
    "        )\n",
    "        print(f\"Frames successfully extracted to {output_folder}\")\n",
    "        return [os.path.join(output_folder, f) for f in sorted(os.listdir(output_folder)) if f.endswith(\".png\")]\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing video {video_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Retrieve transcript of video\n",
    "def get_video_transcript(video_id):\n",
    "    try:\n",
    "        return YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving transcript for video {video_id}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# Generate multimodal embeddings\n",
    "def create_multimodal_embeddings(frames, transcript):\n",
    "    text_inputs = processor(text=transcript, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    image_inputs = processor(images=frames, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        text_features = model.get_text_features(**text_inputs)\n",
    "        image_features = model.get_image_features(**image_inputs)\n",
    "\n",
    "    return text_features, image_features\n",
    "\n",
    "# Download the video and return its file path\n",
    "def download_video(url, options):\n",
    "    with YoutubeDL(options) as ydl:\n",
    "        info_dict = ydl.extract_info(url, download=True)\n",
    "        video_path = ydl.prepare_filename(info_dict)  # Get the path of the downloaded video\n",
    "    return video_path\n",
    "\n",
    "def get_filename(url, options):\n",
    "    with YoutubeDL(options) as ydl:\n",
    "        info_dict = ydl.extract_info(url, download=False)\n",
    "        video_path = ydl.prepare_filename(info_dict)  # Get the path of the downloaded video\n",
    "    return video_path\n",
    "\n",
    "def process_video(video_url, folder='data'):\n",
    "    print(f\"Processing video: {video_url}\")\n",
    "\n",
    "    yt = YouTube(video_url)\n",
    "    video_id = yt.video_id\n",
    "\n",
    "    ydl_opts = {\n",
    "        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',\n",
    "        'outtmpl': os.path.join(folder, '%(title)s.%(ext)s'),\n",
    "        'quiet': True,\n",
    "        'merge_output_format': 'mp4',\n",
    "        'postprocessors': [{'key': 'FFmpegVideoConvertor', 'preferedformat': 'mp4'}]\n",
    "    }\n",
    "    video_title = yt.title\n",
    "    video_path = get_filename(video_url, ydl_opts)\n",
    "    if not os.path.exists(video_path):\n",
    "        video_path = download_video(video_url, ydl_opts)\n",
    "    print(f\"Downloaded video path: {video_path}\")\n",
    "\n",
    "    # Extract frames\n",
    "    frame_folder = os.path.join(folder, video_id)\n",
    "    if not os.path.exists(frame_folder):\n",
    "        os.makedirs(frame_folder)\n",
    "        frames = video_to_images(video_path, frame_folder)\n",
    "    else:\n",
    "        frames = [os.path.join(frame_folder, f) for f in sorted(os.listdir(frame_folder)) if f.endswith(\".png\")]\n",
    "\n",
    "    if not frames:\n",
    "        print(f\"No frames extracted for video {video_url}. Skipping.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Get transcript\n",
    "    transcript = get_video_transcript(video_id)\n",
    "    if not transcript:\n",
    "        print(f\"No transcript available for video {video_url}. Skipping.\")\n",
    "        return None, None, video_title\n",
    "\n",
    "    return transcript, frames, video_title\n",
    "\n",
    "def chunk_transcript(transcript, max_length=1000):\n",
    "    \"\"\"\n",
    "    Chunk transcript into smaller pieces for better indexing.\n",
    "\n",
    "    Parameters:\n",
    "    transcript (str): Full transcript text.\n",
    "    max_length (int): Maximum length of each chunk.\n",
    "\n",
    "    Returns:\n",
    "    list of dict: List of chunks with their corresponding timestamps.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    start_time = None\n",
    "\n",
    "    for entry in transcript:\n",
    "        if current_length == 0:\n",
    "            start_time = entry[\"start\"]  # Use the start time of the first word in the chunk\n",
    "        current_chunk.append(entry[\"text\"])\n",
    "        current_length += len(entry[\"text\"].split())  # Count words in the text\n",
    "\n",
    "        if current_length >= max_length:\n",
    "            chunks.append({\n",
    "                \"text\": \" \".join(current_chunk),\n",
    "                \"timestamp\": f\"{start_time:.2f}s\"  # Format start time as seconds\n",
    "            })\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "\n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append({\n",
    "            \"text\": \" \".join(current_chunk),\n",
    "            \"timestamp\": f\"{start_time:.2f}s\"\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def store_embeddings_in_pinecone(frame_paths, transcript, video_id, index='video-rag', batch_size=16):\n",
    "    \"\"\"\n",
    "    Store embeddings in a single Pinecone namespace with metadata for filtering.\n",
    "\n",
    "    Parameters:\n",
    "    frame_paths (list of str): Paths to extracted frames.\n",
    "    transcript (str): Transcript text.\n",
    "    video_id (str): Unique identifier for the video.\n",
    "    batch_size (int): Number of frames to process per batch.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load all images\n",
    "        images = [Image.open(frame_path) for frame_path in frame_paths]\n",
    "\n",
    "        # Generate and store text embedding\n",
    "        chunks = chunk_transcript(transcript, max_length=200)  # Split transcript into chunks\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            text_inputs = processor(text=chunk[\"text\"], return_tensors=\"pt\", padding=True, truncation=True)\n",
    "            with torch.no_grad():\n",
    "                text_features = model.get_text_features(**text_inputs).cpu().numpy()\n",
    "\n",
    "            metadata = {\n",
    "                \"type\": \"text\",\n",
    "                \"video_id\": video_id,\n",
    "                \"timestamp\": chunk[\"timestamp\"],\n",
    "                \"content\": chunk[\"text\"]\n",
    "            }\n",
    "            index.upsert([(f\"text-{video_id}-{i}\", text_features[0], metadata)])\n",
    "\n",
    "        # Generate and store image embeddings in batches\n",
    "        for i in range(0, len(images), batch_size):\n",
    "            image_batch = images[i:i + batch_size]\n",
    "            image_inputs = processor(images=image_batch, return_tensors=\"pt\", padding=True)\n",
    "            with torch.no_grad():\n",
    "                image_features = model.get_image_features(**image_inputs).cpu().numpy()\n",
    "\n",
    "            # Create metadata for each frame\n",
    "            vectors = []\n",
    "            for j, feature in enumerate(image_features):\n",
    "                frame_metadata = {\n",
    "                    \"type\": \"image\",\n",
    "                    \"video_id\": video_id,\n",
    "                    \"frame_index\": i + j,\n",
    "                    \"timestamp\": f\"{(i + j) * 5}s\"  # Assuming 1 frame per 5 seconds\n",
    "                }\n",
    "                vectors.append((f\"image-{video_id}-{i + j}\", feature, frame_metadata))\n",
    "\n",
    "            index.upsert(vectors)\n",
    "\n",
    "        print(f\"Stored embeddings for video: {video_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing embeddings for video {video_id}: {e}\")\n",
    "\n",
    "\n",
    "# Process all videos in a playlist\n",
    "def process_playlist(playlist_url):\n",
    "    index = init_pinecone('video-rag')\n",
    "    video_ids = get_playlist_videos(playlist_url)\n",
    "    data_folder = '/content/drive/MyDrive/Video_RAG/Data'\n",
    "    if not os.path.exists(data_folder):\n",
    "        os.makedirs(data_folder)\n",
    "    for video_id in video_ids:\n",
    "        try:\n",
    "            print(\"Processing:\"+str(video_id))\n",
    "            trans, frms = process_video(f\"https://www.youtube.com/watch?v={video_id}\", data_folder)\n",
    "            store_embeddings_in_pinecone(frms, trans, video_id, index)\n",
    "            print(\"Embeddings created for \"+str(video_id))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing video {video_id}: {e}\")\n",
    "    return video_ids\n",
    "\n",
    "playlist_url = \"https://www.youtube.com/playlist?list=PL6299F3195349CCDA\"\n",
    "# Process playlist\n",
    "video_ids = process_playlist(playlist_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pinecone\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "\n",
    "# CLIP initialization\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "def query_pinecone_with_timestamps(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Query Pinecone to fetch relevant embeddings and metadata based on the question.\n",
    "    \"\"\"\n",
    "    # Generate query embedding\n",
    "    question_inputs = processor(text=question, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        query_embedding = model.get_text_features(**question_inputs).cpu().numpy()\n",
    "\n",
    "    # Query Pinecone for relevant embeddings\n",
    "    results = index.query(vector=query_embedding[0].tolist(), top_k=top_k, include_metadata=True, include_values=True)\n",
    "    # Extract relevant context\n",
    "    context_segments = []\n",
    "    for match in results[\"matches\"]:\n",
    "        metadata = match[\"metadata\"]\n",
    "        if metadata[\"type\"] == \"text\":\n",
    "            context_segments.append({\n",
    "                \"type\": \"text\",\n",
    "                \"content\": metadata.get(\"content\"),\n",
    "                \"timestamp\": metadata.get(\"timestamp\"),\n",
    "                \"video_id\": metadata.get(\"video_id\")\n",
    "            })\n",
    "        elif metadata[\"type\"] == \"image\":\n",
    "            context_segments.append({\n",
    "                \"type\": \"image\",\n",
    "                \"frame_index\": metadata.get(\"frame_index\"),\n",
    "                \"video_id\": metadata.get(\"video_id\"),\n",
    "                \"timestamp\": metadata.get(\"timestamp\")\n",
    "            })\n",
    "    return context_segments\n",
    "\n",
    "def generate_gpt4_answer(question, context_segments, frame_folder):\n",
    "    \"\"\"\n",
    "    Generate an answer using OpenAI GPT-4 based on the retrieved context.\n",
    "    \"\"\"\n",
    "    # Build the prompt with text and image descriptions\n",
    "    prompt = f\"Question: {question}\\n\\nRelevant Context:\\n\"\n",
    "    for segment in context_segments:\n",
    "        if segment[\"type\"] == \"text\":\n",
    "            prompt += f\"[Text at {segment['timestamp']}]: {segment['content']}\\n\"\n",
    "        elif segment[\"type\"] == \"image\":\n",
    "            # Generate a description of the image (simplified here for illustration)\n",
    "            frame_folder = os.path.join(frame_folder, segment['video_id'])\n",
    "            image_path = f\"{frame_folder}/frame{segment['frame_index']:04d}.png\"\n",
    "            prompt += f\"[Image at {segment['timestamp']}]: See the frame extracted from the video.\\n\"\n",
    "\n",
    "    prompt += \"\\nAnswer:\"\n",
    "\n",
    "    # Query GPT-4 to generate an answer\n",
    "    response = openai.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that answers questions based on video frames and transcripts.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Workflow\n",
    "question = \"What are life history traits?\"\n",
    "\n",
    "# Query Pinecone\n",
    "context_segments = query_pinecone_with_timestamps(question)\n",
    "\n",
    "# Generate Answer\n",
    "frame_folder = 'Data'\n",
    "answer = generate_gpt4_answer(question, context_segments, frame_folder)\n",
    "\n",
    "print(\"Generated Answer:\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
